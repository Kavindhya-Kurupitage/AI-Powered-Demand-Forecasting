# -*- coding: utf-8 -*-
"""AI-POWERED DEMAND FORECASTING .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tM9BlLD_mQG_YgL0JRgAuKjyOlotHMAx

## **PART 01**
"""

!pip install statsmodels --quiet
!pip install openpyxl --quiet
!pip install beautifulsoup4 --quiet

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.tsa.statespace.sarimax import SARIMAX
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import warnings
warnings.filterwarnings('ignore')

"""**Data Loading**"""

sales_df = pd.read_excel("sales_data.xlsx")
inventory_df = pd.read_excel("inventory_data.xlsx")

sales_head = sales_df.head()
inventory_head = inventory_df.head()

sales_info = sales_df.info()
inventory_info = inventory_df.info()

sales_describe = sales_df.describe()
inventory_describe = inventory_df.describe()

sales_head, inventory_head, sales_describe, inventory_describe

"""**Data Cleaning**"""

# Convert to long format
id_cols = ['Category', 'Product', 'Code', 'Size']
date_cols = sales_df.columns.difference(id_cols)

sales_long = sales_df.melt(id_vars=id_cols, value_vars=date_cols,
                           var_name='Date', value_name='Sales')

# Convert Date to datetime and drop NaNs
sales_long['Date'] = pd.to_datetime(sales_long['Date'], errors='coerce')
sales_long.dropna(subset=['Product', 'Size', 'Sales'], inplace=True)

# Filter out any negative sales
sales_long = sales_long[sales_long['Sales'] >= 0]

# Preview cleaned long-format data
sales_long.head()

print(sales_long.columns)

sales_long['Month'] = sales_long['Date'].dt.to_period('M')
monthly_sales = sales_long.groupby(['Product', 'Size', 'Month'])['Sales'].sum().reset_index()

monthly_sales.head()

"""**Forecasting using SARIMAX for a sample product-size**"""

# Using sample product and size
sample_product = 'ALF Boys Ink Trousers'
sample_size = '100'

# Filter data for the selected product and size
filtered_sales = monthly_sales[
    (monthly_sales['Product'] == sample_product) &
    (monthly_sales['Size'] == sample_size)
].copy()

product = 'ALF Boys Ink Trousers'
size = '100'

sales_long['Month'] = sales_long['Date'].dt.to_period('M')
filtered_sales = sales_long[(sales_long['Product'] == product) & (sales_long['Size'] == size)]

filtered_sales.set_index('Month', inplace=True)
filtered_sales.index = filtered_sales.index.to_timestamp()
ts_data = filtered_sales['Sales'].asfreq('MS').fillna(0)

#Visualize to confirm it works
ts_data.plot(title=f'Sales Trend - {product}, Size {size}', figsize=(10, 4), marker='o')

sales_long['Month'] = sales_long['Date'].dt.to_period('M')
filtered_sales = sales_long[(sales_long['Product'] == product) & (sales_long['Size'] == size)]

# Set index to datetime for time series modeling
filtered_sales.set_index('Month', inplace=True)
filtered_sales.index = filtered_sales.index.to_timestamp()
ts_data = filtered_sales['Sales'].asfreq('MS').fillna(0)

sarima_model = SARIMAX(ts_data, order=(1,1,1), seasonal_order=(1,1,1,12))
sarima_result = sarima_model.fit(disp=False)

# Forecast for next 6 months
forecast_steps = 6
forecast = sarima_result.get_forecast(steps=forecast_steps)
forecast_index = pd.date_range(start=ts_data.index[-1] + pd.DateOffset(months=1), periods=forecast_steps, freq='MS')
forecast_mean = forecast.predicted_mean

def sarima_forecast_plot(df, product, size, steps=6):
    sub_df = df[(df['Product'] == product) & (df['Size'] == size)].copy()
    sub_df.set_index('Month', inplace=True)
    sub_df.index = sub_df.index.to_timestamp()
    ts = sub_df['Sales'].asfreq('MS').fillna(0)

    model = SARIMAX(ts, order=(1,1,1), seasonal_order=(1,1,1,12))
    result = model.fit(disp=False)
    forecast = result.get_forecast(steps=steps)
    forecast_mean = forecast.predicted_mean
    forecast_index = pd.date_range(start=ts.index[-1] + pd.DateOffset(months=1), periods=steps, freq='MS')

    plt.figure(figsize=(10, 5))
    plt.plot(ts, label='Historical Sales')
    plt.plot(forecast_index, forecast_mean, label='Forecast', linestyle='--', color='red')
    plt.title(f'SARIMA Forecast for {product} - Size {size}')
    plt.xlabel('Date')
    plt.ylabel('Sales')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

# Forecast and plot for three products/sizes
sarima_forecast_plot(monthly_sales, 'ALF Boys Ink Trousers', '100')
sarima_forecast_plot(monthly_sales, 'ALF Boys Ink Trousers', '108')
sarima_forecast_plot(monthly_sales, 'ALF Boys Ink Trousers', '112')

"""**Order recommendation**"""

from statsmodels.tsa.statespace.sarimax import SARIMAX

# Empty list to collect forecasts
all_forecasts = []

# Unique product-size pairs
product_size_pairs = monthly_sales[['Product', 'Size']].drop_duplicates()

# Forecast next 6 months for each pair
for _, row in product_size_pairs.iterrows():
    product = row['Product']
    size = row['Size']
    sub_df = monthly_sales[(monthly_sales['Product'] == product) & (monthly_sales['Size'] == size)].copy()
    sub_df.set_index('Month', inplace=True)
    sub_df.index = sub_df.index.to_timestamp()
    ts = sub_df['Sales'].asfreq('MS').fillna(0)

    try:
        model = SARIMAX(ts, order=(1,1,1), seasonal_order=(1,1,1,12))
        result = model.fit(disp=False)
        forecast = result.get_forecast(steps=6)
        forecast_mean = forecast.predicted_mean

        for date, value in forecast_mean.items():
            all_forecasts.append({
                'Product': product,
                'Size': size,
                'ForecastMonth': date.strftime('%Y-%m'),
                'ForecastSales': round(value)
            })
    except:
        continue

# Create forecast_df
forecast_df = pd.DataFrame(all_forecasts)
forecast_df.head()

#Aggregate total forecast for next 6 months per product-size
forecast_agg = forecast_df.groupby(['Product', 'Size'])['ForecastSales'].sum().reset_index()
forecast_agg.rename(columns={'ForecastSales': 'TotalForecast6Mo'}, inplace=True)

#Merge with current inventory and lead time
inventory_data = pd.read_excel("inventory_data.xlsx")
order_df = pd.merge(forecast_agg, inventory_data, on=['Product', 'Size'], how='left')

#Calculate order quantity
order_df['OrderQty'] = order_df['TotalForecast6Mo'] - order_df['Stock On Hand']
order_df['OrderQty'] = order_df['OrderQty'].apply(lambda x: max(0, round(x)))

order_df['TotalForecast6Mo'] = order_df['TotalForecast6Mo'].round(0)

order_df.to_csv("forecast_and_orders.csv", index=False)

"""## **PART 02**"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import re

school_urls = {
    "Alfriston College": "https://www.educationcounts.govt.nz/find-school/school/profile?school=6977",
    "Western Heights High": "https://www.educationcounts.govt.nz/find-school/school/profile?school=140",
    "Mt Albert Grammar": "https://www.educationcounts.govt.nz/find-school/school/profile?school=67"
}

#Etract the information
def extract_school_data(school_name, url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')
    text = soup.get_text()

    def find_value(label):
        pattern = rf"{label}[\s:]*([A-Za-z0-9,\s\-â€“]+)"
        match = re.search(pattern, text)
        return match.group(1).strip() if match else "Not found"

    return {
        "School Name": school_name,
        "Roll": find_value("School Roll"),
        "Decile": find_value("School Decile") or find_value("Equity Index"),
        "Gender": find_value("Gender"),
        "Year Levels": find_value("School Type") or find_value("Year Levels")
    }

# Scrape data
school_data = [extract_school_data(name, url) for name, url in school_urls.items()]

# Convert to DataFrame
school_df = pd.DataFrame(school_data)
print(school_df)

school_df.to_csv("school_data.csv", index=False)

from google.colab import files
files.download("school_data.csv")

"""**XGBoost**"""

#Feature Engineering for XGBoost
sales_long['MonthNum'] = sales_long['Date'].dt.month
sales_long['Year'] = sales_long['Date'].dt.year

# Sort before creating lag features
sales_long.sort_values(['Product', 'Size', 'Date'], inplace=True)

# Create Lag_1
sales_long['Lag_1'] = sales_long.groupby(['Product', 'Size'])['Sales'].shift(1)

# Create Lag_3_Avg correctly
sales_long['Lag_3_Avg'] = sales_long.groupby(['Product', 'Size'])['Sales'].shift(1).rolling(window=3).mean().reset_index(drop=True)

# Drop rows with NaN lag values
xgb_data = sales_long.dropna(subset=['Lag_1', 'Lag_3_Avg'])

#encode product/size as category codes (if too many unique values)
xgb_data['ProductCode'] = xgb_data['Product'].astype('category').cat.codes
xgb_data['SizeCode'] = xgb_data['Size'].astype('category').cat.codes

xgb_data.head()

# Clean and encode school metadata
school_df.rename(columns={"School Name": "Category"}, inplace=True)

# Encode gender and year level as categorical codes
school_df['GenderCode'] = school_df['Gender'].astype('category').cat.codes
school_df['YearLevelCode'] = school_df['Year Levels'].astype('category').cat.codes

# Merge school metadata into sales data
xgb_data = pd.merge(sales_long, school_df, on='Category', how='left')

# Drop any rows that couldn't be matched
xgb_data.dropna(subset=['Roll', 'GenderCode', 'YearLevelCode'], inplace=True)

# Feature columns
feature_cols = ['Lag_1', 'Lag_3_Avg', 'MonthNum', 'Year', 'Roll', 'Decile', 'GenderCode', 'YearLevelCode']

# Target column
target_col = 'Sales'

# Drop NaNs
xgb_data = xgb_data.dropna(subset=feature_cols + [target_col])

# Train/test split
from sklearn.model_selection import train_test_split

X = xgb_data[feature_cols]
y = xgb_data[target_col]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Convert to numeric type safely
X_train['Roll'] = pd.to_numeric(X_train['Roll'], errors='coerce')
X_train['Decile'] = pd.to_numeric(X_train['Decile'], errors='coerce')
X_test['Roll'] = pd.to_numeric(X_test['Roll'], errors='coerce')
X_test['Decile'] = pd.to_numeric(X_test['Decile'], errors='coerce')

#Drop rows with any NaN
X_train.dropna(inplace=True)
y_train = y_train.loc[X_train.index]

X_test.dropna(inplace=True)
y_test = y_test.loc[X_test.index]

print(X_train.dtypes)

print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

# Fill missing values with median to avoid empty splits
X = X.fillna(X.median(numeric_only=True))

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)

X['Roll'] = pd.to_numeric(X['Roll'], errors='coerce')
X['Decile'] = pd.to_numeric(X['Decile'], errors='coerce')

X = X.fillna(X.median(numeric_only=True))

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("Check dtypes:")
print(X_train.dtypes)

model = XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=5)
model.fit(X_train, y_train)

y_pred = model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
print("XGBoost RMSE:", rmse)

from dateutil.relativedelta import relativedelta
import datetime


latest_date = xgb_data['Date'].max()
next_months = [latest_date + relativedelta(months=i) for i in range(1, 7)]

forecast_rows = []

grouped = xgb_data.groupby(['Product', 'Size', 'Category'])

for (product, size, school), group in grouped:
    last_row = group.sort_values('Date').iloc[-1]

    for date in next_months:
        forecast_rows.append({
            'Product': product,
            'Size': size,
            'Category': school,
            'Lag_1': last_row['Sales'],
            'Lag_3_Avg': last_row['Lag_3_Avg'],
            'MonthNum': date.month,
            'Year': date.year,
            'Roll': last_row['Roll'],
            'Decile': last_row['Decile'],
            'GenderCode': last_row['GenderCode'],
            'YearLevelCode': last_row['YearLevelCode'],
            'ForecastMonth': date.strftime('%Y-%m')
        })

# Convert to DataFrame
forecast_input = pd.DataFrame(forecast_rows)

# Ensure Roll and Decile are numeric
forecast_input['Roll'] = pd.to_numeric(forecast_input['Roll'], errors='coerce')
forecast_input['Decile'] = pd.to_numeric(forecast_input['Decile'], errors='coerce')
forecast_input = forecast_input.fillna(forecast_input.median(numeric_only=True))

# Predict with XGBoost model
X_forecast = forecast_input[feature_cols]
forecast_input['ForecastSales'] = model.predict(X_forecast).round()

# Display the forecast
forecast_input[['Product', 'Size', 'Category', 'ForecastMonth', 'ForecastSales']].head(12)

forecast_input.to_csv("xgboost_forecast_6mo.csv", index=False)
from google.colab import files
files.download("xgboost_forecast_6mo.csv")